{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzus3lH0hV7DKWfFwTNTLQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saranyapichandi96/task8/blob/main/task8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task-1\n",
        "You need to translate each word or sentence from English to Spanish, French and German \n",
        "\n"
      ],
      "metadata": {
        "id": "O4PYg6Rk1v9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "data = read_csv(\"English.csv\")\n",
        "deu_eng = to_lines(data)\n",
        "deu_eng = array(deu_eng)\n",
        "#Data preprocessing\n",
        "deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\n",
        "deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n",
        "# convert text to lowercase\n",
        "for i in range(len(deu_eng)):\n",
        "    deu_eng[i,0] = deu_eng[i,0].lower()\n",
        "    deu_eng[i,1] = deu_eng[i,1].lower()\n",
        "# empty lists\n",
        "eng_l = []\n",
        "deu_l = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in deu_eng[:,0]:\n",
        "      eng_l.append(len(i.split()))\n",
        "\n",
        "for i in deu_eng[:,1]:\n",
        "      deu_l.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'eng':eng_l, 'deu':deu_l})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(deu_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "# prepare Deutch tokenizer\n",
        "deu_tokenizer = tokenization(deu_eng[:, 1])\n",
        "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
        "\n",
        "deu_length = 8\n",
        "print('Deutch Vocabulary Size: %d' % deu_vocab_size)\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split data into train and test set\n",
        "train, test = train_test_split(deu_eng, test_size=0.2, random_state = 12)\n",
        "# prepare training data\n",
        "trainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "\n",
        "# build NMT model\n",
        "def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "      model.add(LSTM(units))\n",
        "      model.add(RepeatVector(out_timesteps))\n",
        "      model.add(LSTM(units, return_sequences=True))\n",
        "      model.add(Dense(out_vocab, activation='softmax'))\n",
        "      return model\n",
        "# model compilation\n",
        "model = define_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
        "filename = 'TRANSLATION'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# train model\n",
        "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
        "                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n",
        "                    verbose=1)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()\n",
        "model = load_model('TRANSLATION')\n",
        "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))  \n",
        "def get_word(n, tokenizer):\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "          if index == n:\n",
        "              return word\n",
        "      return None\n",
        "preds_text = []\n",
        "for i in preds:\n",
        "       temp = []\n",
        "       for j in range(len(i)):\n",
        "            t = get_word(i[j], eng_tokenizer)\n",
        "            if j > 0:\n",
        "                if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                     temp.append('')\n",
        "                else:\n",
        "                     temp.append(t)\n",
        "            else:\n",
        "                   if(t == None):\n",
        "                          temp.append('')\n",
        "                   else:\n",
        "                          temp.append(t) \n",
        "\n",
        "       preds_text.append(' '.join(temp))\n",
        "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\n",
        "# print 15 rows randomly\n",
        "pred_df.sample(15)         "
      ],
      "metadata": {
        "id": "y4wXEWD91vhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjBHytbvJAC6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#sklearn\n",
        "from sklearn import preprocessing, model_selection, decomposition, pipeline, metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold,RandomizedSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "#pip install xgboost\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"English.csv\",column='words')"
      ],
      "metadata": {
        "id": "_X5OUhraLjEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "FRuAJqkQL9d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "o3hLbsi7MHp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "data['English words/sentences']=data['English words/sentences'].apply(lambda x:clean_text(x))\n",
        "data['English words/sentences'][:5]"
      ],
      "metadata": {
        "id": "WkWKfCWyMLeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.value_counts()"
      ],
      "metadata": {
        "id": "RWBbsujuMuzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task-2\n",
        "\tCreate a program that needs to automatically correct that spelling from the word or a given sentence. (Language : English) "
      ],
      "metadata": {
        "id": "EGfCMQ1JQAdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install autocorrect"
      ],
      "metadata": {
        "id": "Xc8RfHAUOozq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autocorrect import Speller\n",
        "spell = Speller(lang = 'en')"
      ],
      "metadata": {
        "id": "6UoPKEo2NJgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spell(tokens):\n",
        "    senten_corrected = ' '.join([spell(word) for word in tokens])\n",
        "    return senten_corrected\n",
        "\n",
        "data['English words/sentences']=data['English words/sentences'].apply(lambda x:correct_spell(x))\n",
        "#data['English words/sentences'][:5]\n",
        "data.head()"
      ],
      "metadata": {
        "id": "e7oYPQFpOlH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task-3\n",
        "\tCreate an application that should be used by the HR Team to filter the resume based on the Skills.\n"
      ],
      "metadata": {
        "id": "QcBfMcl7VQGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "#Loading Data\n",
        "resumeDataSet = pd.read_csv('../input/ResumeScreeningDataSet.csv' ,encoding='utf-8')\n",
        "#EDA\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.xticks(rotation=90)\n",
        "sns.countplot(y=\"Category\", data=resumeDataSet)\n",
        "plt.savefig('../output/jobcategory_details.png')\n",
        "#Pie-chart\n",
        "targetCounts = resumeDataSet['Category'].value_counts().reset_index()['Category']\n",
        "targetLabels  = resumeDataSet['Category'].value_counts().reset_index()['index']\n",
        "# Make square figures and axes\n",
        "plt.figure(1, figsize=(25,25))\n",
        "the_grid = GridSpec(2, 2)\n",
        "plt.subplot(the_grid[0, 1], aspect=1, title='CATEGORY DISTRIBUTION')\n",
        "source_pie = plt.pie(targetCounts, labels=targetLabels, autopct='%1.1f%%', shadow=True, )\n",
        "plt.savefig('../output/category_dist.png')\n",
        "#Data Preprocessing\n",
        "def cleanResume(resumeText):\n",
        "    resumeText = re.sub('httpS+s*', ' ', resumeText)  # remove URLs\n",
        "    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n",
        "    resumeText = re.sub('#S+', '', resumeText)  # remove hashtags\n",
        "    resumeText = re.sub('@S+', '  ', resumeText)  # remove mentions\n",
        "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
        "    resumeText = re.sub(r'[^x00-x7f]',r' ', resumeText) \n",
        "    resumeText = re.sub('s+', ' ', resumeText)  # remove extra whitespace\n",
        "    return resumeText\n",
        "resumeDataSet['cleaned_resume'] = resumeDataSet.Resume.apply(lambda x: cleanResume(x))\n",
        "var_mod = ['Category']\n",
        "le = LabelEncoder()\n",
        "for i in var_mod:\n",
        "    resumeDataSet[i] = le.fit_transform(resumeDataSet[i])\n",
        "requiredText = resumeDataSet['cleaned_resume'].values\n",
        "requiredTarget = resumeDataSet['Category'].values\n",
        "word_vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    stop_words='english',\n",
        "    max_features=1500)\n",
        "word_vectorizer.fit(requiredText)\n",
        "WordFeatures = word_vectorizer.transform(requiredText)\n",
        "X_train,X_test,y_train,y_test = train_test_split(WordFeatures,requiredTarget,random_state=0, test_size=0.2)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "clf = OneVsRestClassifier(KNeighborsClassifier())\n",
        "clf.fit(X_train, y_train)\n",
        "prediction = clf.predict(X_test)\n",
        "print('Accuracy of KNeighbors Classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\n",
        "print('Accuracy of KNeighbors Classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))\n",
        "print(metrics.classification_report(y_test, prediction))"
      ],
      "metadata": {
        "id": "DVqgzv88WbKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task-4\n",
        "Create a chatbot for Hotel Management to Book Rooms \n",
        "\n"
      ],
      "metadata": {
        "id": "sKJJw9_21knf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Text, Dict, List, Union\n",
        "\n",
        "from rasa_sdk import Action, Tracker\n",
        "from rasa_sdk.events import SlotSet, UserUtteranceReverted\n",
        "from rasa_sdk.executor import CollectingDispatcher\n",
        "from rasa_sdk.forms import FormAction\n",
        "\n",
        "def repeat(tracker, dispatcher):\n",
        "    user_ignore_count = 2\n",
        "    count = 0\n",
        "    tracker_list = []\n",
        "\n",
        "    while user_ignore_count > 0:\n",
        "        event = tracker.events[count].get('event')\n",
        "        if event == 'user':\n",
        "            user_ignore_count = user_ignore_count - 1\n",
        "        if event == 'bot':\n",
        "            tracker_list.append(tracker.events[count])\n",
        "        count = count - 1\n",
        "\n",
        "    tracker_list.reverse()\n",
        "    i = len(tracker_list) - 1\n",
        "\n",
        "    while i >= 0:\n",
        "        data = tracker_list[i].get('data')\n",
        "        if data:\n",
        "            if \"buttons\" in data:\n",
        "                dispatcher.utter_message(text=tracker_list[i].get('text'), buttons=data[\"buttons\"])\n",
        "            else:\n",
        "                dispatcher.utter_message(text=tracker_list[i].get('text'))\n",
        "            break\n",
        "        i -= 1\n",
        "\n",
        "class BookRoomInfo(FormAction):\n",
        "    def name(self) -> Text:\n",
        "        return \"form_book_room\"\n",
        "\n",
        "    @staticmethod\n",
        "    def required_slots(tracker: Tracker) -> List[Text]:\n",
        "        return [\"number\", \"room_type\"]\n",
        "\n",
        "    def submit(\n",
        "            self,\n",
        "            dispatcher: CollectingDispatcher,\n",
        "            tracker: Tracker,\n",
        "            domain: Dict[Text, Any],\n",
        "    ) -> List[Dict]:\n",
        "\n",
        "        # utter submit template\n",
        "        dispatcher.utter_message(template=\"utter_submit\", number=tracker.get_slot('number'),\n",
        "                                 room_type=tracker.get_slot('room_type'))\n",
        "        return []\n",
        "\n",
        "    def slot_mappings(self) -> Dict[Text, Union[Dict, List[Dict]]]:\n",
        "\n",
        "        return {\n",
        "            \"number\": self.from_entity(entity=\"number\", intent='num_rooms'),\n",
        "            \"room_type\": self.from_entity(entity=\"room_type\", intent=\"type_rooms\")\n",
        "        }\n",
        "\n",
        "class BookRoomNumberInfo(FormAction):\n",
        "    def name(self) -> Text:\n",
        "        return \"form_book_room_number\"\n",
        "\n",
        "    @staticmethod\n",
        "    def required_slots(tracker: Tracker) -> List[Text]:\n",
        "        return [\"room_type\"]\n",
        "\n",
        "    def submit(\n",
        "            self,\n",
        "            dispatcher: CollectingDispatcher,\n",
        "            tracker: Tracker,\n",
        "            domain: Dict[Text, Any],\n",
        "    ) -> List[Dict]:\n",
        "\n",
        "        # utter submit template\n",
        "        dispatcher.utter_message(template=\"utter_submit\", \n",
        "                                room_type=tracker.get_slot('room_type'))\n",
        "        return []\n",
        "\n",
        "    def slot_mappings(self) -> Dict[Text, Union[Dict, List[Dict]]]:\n",
        "\n",
        "        return {\n",
        "            \"room_type\": self.from_entity(entity=\"room_type\", intent=\"type_rooms\")\n",
        "        }\n",
        "\n",
        "class ResetSlots(Action):\n",
        "\n",
        "    def name(self):\n",
        "        return \"action_reset_slots\"\n",
        "\n",
        "    def run(self, dispatcher, tracker, domain):\n",
        "        return [SlotSet(\"number\", None), SlotSet(\"room_type\", None)]\n",
        "\n",
        "class MyFallbackAction(Action):\n",
        "    def name(self):\n",
        "        return \"action_my_fallback\"\n",
        "\n",
        "    def run(self, dispatcher, tracker, domain):\n",
        "        dispatcher.utter_template(\"utter_fallback_message\", tracker)\n",
        "        # repeat(tracker, dispatcher)        \n",
        "        return [UserUtteranceReverted()]\n",
        "\n",
        "class ActionCheckInTime(Action):\n",
        "\n",
        "    def name(self):\n",
        "        return \"action_check_in_time\"\n",
        "\n",
        "    def run(self, dispatcher, tracker, domain):\n",
        "        dispatcher.utter_template(\"utter_check_in_time\", tracker)\n",
        "        repeat(tracker, dispatcher)        \n",
        "        return [UserUtteranceReverted()]\n",
        "\n",
        "class ActionCheckOutTime(Action):\n",
        "    \n",
        "    def name(self):\n",
        "        return \"action_check_out_time\"\n",
        "\n",
        "    def run(self, dispatcher, tracker, domain):\n",
        "        dispatcher.utter_template(\"utter_check_out_time\", tracker)\n",
        "        repeat(tracker, dispatcher)\n",
        "        return [UserUtteranceReverted()]\n",
        "    \n",
        "class ActionCancelReservation(Action):\n",
        "    \n",
        "    def name(self):\n",
        "        return \"action_cancel_reservation\"\n",
        "\n",
        "    def run(self, dispatcher, tracker, domain):\n",
        "        dispatcher.utter_template(\"utter_cancel_reservation\", tracker)\n",
        "        repeat(tracker, dispatcher)\n",
        "        return [UserUtteranceReverted()]\n",
        "\n",
        "class ActionCancellationPolicy(Action):\n",
        "    \n",
        "    def name(self):\n",
        "        return \"action_cancellation_policy\"\n",
        "\n",
        "    def run(self, dispatcher, tracker, domain):\n",
        "        dispatcher.utter_template(\"utter_cancellation_policy\", tracker)\n",
        "        repeat(tracker, dispatcher)\n",
        "        return [UserUtteranceReverted()]\n",
        "\n",
        "class ActionHaveRestaurant(Action):\n",
        "    \n",
        "    def name(self):\n",
        "        return \"action_have_restaurant\"\n",
        "\n",
        "    def run(self, dispatcher, tracker, domain):\n",
        "        dispatcher.utter_template(\"utter_have_restaurant\", tracker)\n",
        "        repeat(tracker, dispatcher)\n",
        "        return [UserUtteranceReverted()]\n",
        "\n",
        "class ActionBreakfastAvail(Action):\n",
        "    \n",
        "    def name(self):\n",
        "        return \"action_breakfast_avail\"\n",
        "\n",
        "    def run(self, dispatcher, tracker, domain):\n",
        "        dispatcher.utter_template(\"utter_breakfast_avail\", tracker)\n",
        "        repeat(tracker, dispatcher)\n",
        "        return [UserUtteranceReverted()]\n",
        "    \n",
        "class ActionBreakfastTime(Action):\n",
        "    \n",
        "    def name(self):\n",
        "        return \"action_breakfast_time\"\n",
        "\n",
        "    def run(self, dispatcher, tracker, domain):\n",
        "        dispatcher.utter_template(\"utter_breakfast_time\", tracker)\n",
        "        repeat(tracker, dispatcher)\n",
        "        return [UserUtteranceReverted()]\n",
        "\n",
        "class ActionRestaurantTime(Action):\n",
        "    \n",
        "    def name(self):\n",
        "        return \"action_restaurant_time\"\n",
        "\n",
        "    def run(self, dispatcher, tracker, domain):\n",
        "        dispatcher.utter_template(\"utter_restaurant_time\", tracker)\n",
        "        repeat(tracker, dispatcher)\n",
        "        return [UserUtteranceReverted()]"
      ],
      "metadata": {
        "id": "FfmX-F1ayLHu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}